{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Namu Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 <ul> blocks right under that div.\n",
      "Done! Wrote 'paths.js' with keys 1..32.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import copy\n",
    "\n",
    "def main():\n",
    "    url = \"https://en.namu.wiki/w/세피로트의%20나무#s-3.6\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    html = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # 1) Find <h3> with <a id=\"s-3.6\">\n",
    "    anchor = soup.find(\"a\", id=\"s-3.6\")\n",
    "    if not anchor:\n",
    "        raise RuntimeError(\"Could not locate <a id='s-3.6'> anchor.\")\n",
    "\n",
    "    h3_tag = anchor.find_parent(\"h3\")\n",
    "    if not h3_tag:\n",
    "        raise RuntimeError(\"Could not locate the <h3> that wraps anchor #s-3.6\")\n",
    "\n",
    "    # 2) The immediate next sibling div\n",
    "    main_div = h3_tag.find_next_sibling(\"div\")\n",
    "    if not main_div:\n",
    "        raise RuntimeError(\"Could not find next-sibling <div> after <h3>\")\n",
    "\n",
    "    # 3) Top-level <ul> blocks under that div\n",
    "    all_uls = main_div.find_all(\"ul\", recursive=False)\n",
    "    print(f\"Found {len(all_uls)} <ul> blocks right under that div.\")\n",
    "\n",
    "    # Regex: \"Sephira 6 (Kether)\" or \"Path 12: Burning Raging\"\n",
    "    # Case-insensitive via (?i) at start\n",
    "    title_pattern = re.compile(\n",
    "        r\"(?i)^\\s*(sephira|path)\\s+(\\d+)\\s*\"\n",
    "        r\"(?:\\(\\s*(.*?)\\s*\\)|:\\s*(.*))?\\s*$\"\n",
    "    )\n",
    "\n",
    "    def parse_li_for_data(li):\n",
    "        \"\"\"\n",
    "        1) Convert the LI to text and parse the first line with a regex \n",
    "           for 'Sephira 6 (Kether)' or 'Path 12: Burning Raging'.\n",
    "        2) If name not found, see if there's a <strong> we can use.\n",
    "        3) Remove that <strong> tag from the DOM if used.\n",
    "        4) Also remove the entire 'first line' from the final description text \n",
    "           so we don't duplicate the type/name in the description.\n",
    "        \"\"\"\n",
    "\n",
    "        # Full text, used only for the first-line pattern check\n",
    "        full_text = li.get_text(separator=\"\\n\\n\", strip=True)\n",
    "        paragraphs = full_text.split(\"\\n\\n\")\n",
    "        first_line = paragraphs[0] if paragraphs else \"\"\n",
    "        description_paragraphs = paragraphs[1:]  # by default skip the first line\n",
    "\n",
    "        # Attempt to parse \"Sephira 1 (Kether)\" or \"Path 12: Something\"\n",
    "        m = title_pattern.match(first_line)\n",
    "        if m:\n",
    "            item_type_raw = m.group(1)  # \"sephira\"/\"path\"\n",
    "            number_str    = m.group(2)  # e.g. \"6\" or \"11\"\n",
    "            name_paren    = (m.group(3) or \"\").strip()  # from (...)\n",
    "            name_colon    = (m.group(4) or \"\").strip()  # from : ...\n",
    "            raw_name = name_paren if name_paren else name_colon\n",
    "        else:\n",
    "            # If we cannot parse it at all, \n",
    "            # store everything in description and return empty type/name\n",
    "            return (\"\", \"\", full_text)\n",
    "\n",
    "        # Build final \"title\": e.g. \"Sephira 6\" or \"Path 11\"\n",
    "        raw_type = f\"{item_type_raw} {number_str}\"\n",
    "        item_title = raw_type.title()   # e.g. \"Sephira 6\" / \"Path 11\"\n",
    "\n",
    "        # If we didn't get a name from parentheses/colon, try <strong>\n",
    "        if not raw_name:\n",
    "            strong_tag = li.find(\"strong\")\n",
    "            if strong_tag:\n",
    "                raw_name = strong_tag.get_text(strip=True)\n",
    "                # remove it from the DOM so it's not repeated in the final description\n",
    "                strong_tag.extract()\n",
    "\n",
    "        item_name = raw_name.title() if raw_name else \"\"\n",
    "\n",
    "        # Now that we've possibly removed <strong>, let's get the updated text \n",
    "        # without that <strong> or the first line. We'll do a fresh extraction:\n",
    "        li_copy = copy.copy(li)  # shallow copy of the element\n",
    "        # but we also want to remove the entire first line from li_copy's text\n",
    "\n",
    "        # A simple approach: \n",
    "        #  - Remove the entire first line node. Typically that line is in a <div> or text node.\n",
    "        #    or we can re-get the text, then skip the first paragraph.\n",
    "\n",
    "        # We'll just re-get the text from li (with <strong> removed if used),\n",
    "        # then skip the first paragraph.\n",
    "        updated_text = li.get_text(separator=\"\\n\\n\", strip=True)\n",
    "        updated_paragraphs = updated_text.split(\"\\n\\n\")\n",
    "        # The first paragraph should be \"Path 11: ...\" or \"Sephira 3 (Bina) ...\",\n",
    "        # so we skip it.  The rest is the real description.\n",
    "\n",
    "        final_description = \"\\n\\n\".join(updated_paragraphs[1:])\n",
    "\n",
    "        return (item_title, item_name, final_description)\n",
    "\n",
    "    data_dict = {}\n",
    "\n",
    "    for i, ul_tag in enumerate(all_uls, start=1):\n",
    "        li = ul_tag.find(\"li\", recursive=False)\n",
    "        if not li:\n",
    "            continue\n",
    "\n",
    "        item_title, item_name, item_desc = parse_li_for_data(li)\n",
    "\n",
    "        data_dict[i] = {\n",
    "            \"title\": item_title,\n",
    "            \"name\": item_name,\n",
    "            \"description\": item_desc\n",
    "        }\n",
    "\n",
    "    # 4) Write out \"paths.js\"\n",
    "    with open(\"paths.js\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"const pathsData = \")\n",
    "        f.write(json.dumps(data_dict, ensure_ascii=False, indent=2))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Done! Wrote 'paths.js' with keys 1..{}.\".format(len(data_dict)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
